import { GoogleGenAI, Type } from "@google/genai";
import { AnalysisResult, DetectionVerdict } from "../types";

// PROMPT ENGINEERING:
const SYSTEM_INSTRUCTION = `
You are a forensic image analyst specialized in detecting synthetic media generated by advanced diffusion and transformer models (specifically Google Imagen, Midjourney, and DALL-E).
Your goal is to detect if the input image is AI-generated.

Analyze the image based on these advanced forensic pillars:
1. **Light & Physics Logic**: Check for inconsistent shadow directions, impossible reflections in eyes/mirrors, and light sources that don't match the scene.
2. **Texture & Noise**: Look for "painterly" or "smooth" skin textures that lack pores, background objects that merge into one another, or hair strands that disappear into skin.
3. **Semantic Consistency**: Look for objects that don't make sense functionally (e.g., a bicycle with disconnected chains, gibberish text on signs/clothing, asymmetrical glasses/jewelry).
4. **Digital Sheen**: Look for the characteristic high-contrast, hyper-saturated "glossy" look common in AI generation.
5. **Model-Specific Artifacts**: Look for specific artifacts common to Google AI models (Imagen/Gemini) or others, such as specific text rendering issues or "plastic" smoothing.

Return the result in a strict JSON format.
`;

export const analyzeImageWithGemini = async (base64Image: string, mimeType: string): Promise<AnalysisResult> => {
  try {
    const apiKey = process.env.API_KEY;
    if (!apiKey) {
      throw new Error("API Key is missing");
    }

    const ai = new GoogleGenAI({ apiKey });

    // Using gemini-2.5-flash as requested
    const response = await ai.models.generateContent({
      model: 'gemini-2.5-flash', 
      contents: {
        parts: [
          {
            inlineData: {
              mimeType: mimeType,
              data: base64Image
            }
          },
          {
            text: "Conduct a forensic analysis of this image. List specific artifacts found (if any) and provide a final probability verdict."
          }
        ]
      },
      config: {
        systemInstruction: SYSTEM_INSTRUCTION,
        responseMimeType: "application/json",
        responseSchema: {
          type: Type.OBJECT,
          properties: {
            analysis_points: {
              type: Type.ARRAY,
              description: "List of 3-5 specific visual observations.",
              items: { type: Type.STRING }
            },
            verdict: {
              type: Type.STRING,
              enum: [DetectionVerdict.YES, DetectionVerdict.NO, DetectionVerdict.NOT_SURE],
              description: "Final determination: Is this likely AI-generated?"
            },
            confidence: {
              type: Type.NUMBER,
              description: "Confidence score from 0 to 100."
            },
            reasoning: {
              type: Type.STRING,
              description: "A concise summary of the verdict based on the analysis points."
            }
          },
          required: ["analysis_points", "verdict", "confidence", "reasoning"]
        }
      }
    });

    // FIX: Changed 'response.text()' back to 'response.text' (property access).
    // The @google/genai SDK uses a getter property, not a method.
    const text = response.text; 
    
    if (!text) {
      throw new Error("No response text received from Gemini");
    }

    const result = JSON.parse(text) as AnalysisResult;
    return result;

  } catch (error) {
    console.error("Error analyzing image:", error);
    throw error;
  }
};